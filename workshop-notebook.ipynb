{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Synthetic Data with Python\n",
    "\n",
    "Hand's on workshop covering synthetic data with Python.\n",
    "\n",
    "**Agenda**:\n",
    "- Overview\n",
    "- Bootstrapping\n",
    "- SMOTE\n",
    "- Image Augmentation\n",
    "- Synthetic Data Vault (SDV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### What is synthetic data?\n",
    "\n",
    "Data that is not \"real\", but has been generated to reflect some real world data or process. \n",
    "\n",
    "Data access can be challenging:\n",
    "- Limited resources\n",
    "- Privacy concerns\n",
    "\n",
    "Synthetic data is becoming a prominent strategy to improve data access against both of these cases.\n",
    "\n",
    "\n",
    "### How do we create synthetic data?\n",
    "\n",
    "Two primary methods:\n",
    "1. Simulate real data\n",
    "2. Use existing model or background knowledge\n",
    "\n",
    "![Generation Process](images/Synthetic-data-generation.png)\n",
    "\n",
    "\n",
    "### Key Ideas\n",
    "\n",
    "- **Utility**: how accurately synthetic data reflects real data or process\n",
    "    - Depends on use case\n",
    "- **Anonymization**: ensure protection of sensitive information\n",
    "- **Reproducibiliy**: generation process can be replicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Basic Packages (some others will be imported later)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Classification models/metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Functions from .py file\n",
    "from src.funcs import plot_hist, plot_churn, model_churn, plot_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap\n",
    "\n",
    "Monte Carlo Simulation approach to estimate the uncertainty of a statistic or estimator. Any sample taken from a population includes an inherent error from the randomness of the sampling process itself. We use the Bootstrap method to infer how other samples taken from the same population would have differed due to random error, and thus produce some idea of the uncertainty in our original sample. \n",
    "\n",
    "At a high level the process is as follows:\n",
    "1. Take random sample from a population\n",
    "1. Consider the sample to be the population\n",
    "1. Repeatedly sample *with replacement* from original sample to create *bootstrap samples*\n",
    "\n",
    "\n",
    "In this sense, we can consider each bootstrap sample to be a synthetic set of data points which represent what the original sample *could* have been. We can achieve similar results to the Central Limit Theorem, only using a single sample!\n",
    "\n",
    "Let's look at an example with a dataset below. We will read pandas Series on profits for sales of a store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_profit = pd.read_csv('data/profits.csv')['Profit']\n",
    "pop_profit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks:\n",
    "- Take random sample of n=250\n",
    "- Explore distribution of sample\n",
    "- Take 10,000 bootstrap resamples\n",
    "- Explore distribution of 4 bootstrap resamples\n",
    "- Construct bootstrap sampling distribution\n",
    "- Calculate 90% confidence interval to estimate population mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE\n",
    "\n",
    "Synthetic Minority Over-sampling Technique is a strategy used to address a very common challenge in machine learning: class imbalance. Class imbalance refers to classification problems with one class in the dataset being more prevalent than others. \n",
    "\n",
    "Examples include:\n",
    "- Fraud\n",
    "- Medical scans\n",
    "- Car crashes\n",
    "\n",
    "A model trained on an imbalanced dataset will naturally be biased to predict the majority class. A common method to counteract this issue involves generating synthetic observations of the minority class. We can give the model more data to learn from without investing additional resources to collect real data!\n",
    "\n",
    "The example below looks at a the [Telecom Churn](https://www.kaggle.com/datasets/mnassrib/telecom-churn-datasets?select=churn-bigml-80.csv) dataset. Our goal is to use information on customer behavior to predict whether or not the customer cancels their subscription (churn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read churn csv into DataFrame\n",
    "df = pd.read_csv('data/churn-bigml-80.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks:\n",
    "- Explore distribution of target variable\n",
    "- Train model on original, imbalanced data\n",
    "- Generate synthetic samples of minority class\n",
    "- Train new model and compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation\n",
    "\n",
    "> \"Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data.\" [Berkeley Artifical Intelligence Research](https://bair.berkeley.edu/blog/2019/06/07/data_aug/)\n",
    "\n",
    "Let's look at a simple example with the MNIST dataset on images of hand-written digits. The following example is taken from this [article](https://machinelearningmastery.com/image-augmentation-deep-learning-keras/).\n",
    "\n",
    "#### Tasks:\n",
    "- Load MNIST data\n",
    "- Explore basic images\n",
    "- [Tensorflow Image Augmentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) tool\n",
    "- Create full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MNIST dataset\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Standardize images and convert to float\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore original data\n",
    "plot_grid(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Tensorflow Image Augmentation Tool\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data Vault (SDV)\n",
    "\n",
    ">\"The **Synthetic Data Vault (SDV)** is a **Synthetic Data Generation** ecosystem of libraries that allows users to easily learn single-table, multi-table and timeseries datasets to later on generate new **Synthetic Data** that has the **same format and statistical properties** as the original dataset.\" [Synthetic Data Vault](https://sdv.dev/SDV/index.html)\n",
    "\n",
    "Topics to Discuss:\n",
    "- Fitting model and generating data\n",
    "- Faker and anonymizing sensitive information\n",
    "- Exploring distributions\n",
    "- Constraints and creating your own\n",
    "- Metrics to measure utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (synthetic-data)",
   "language": "python",
   "name": "synthetic-data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
